import re
import itertools
import multiprocessing
 
from pyspark import StorageLevel
from pyspark.sql import functions as F
from pyspark.sql import SparkSession, types
from pyspark.sql.functions import col, isnan, when, count, udf
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
 
from pyspark.ml import Pipeline
from pyspark.ml.regression import GeneralizedLinearRegression
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel
 
train_df = spark.read.parquet('s3://myawsbucketfolder/train.parquet', header=True, inferSchema=True)
label_df = spark.read.csv('s3://myawsbucketfolder/train_labels.csv', header=True, inferSchema=True)
 
def add_suffix(names, suffix):
         return [name + suffix for name in names]
     
# Known Columns
target_df = ['target']
info_df = ['customer_ID', 'S_2']
cat_df = ['B_30', 'B_38','D_114', 'D_116', 'D_117', 'D_120', 'D_126','D_63','D_64','D_66','D_68']

# Define Numeric Columns
excluded = info_df + cat_df
num_df = [col for col in train_df.columns if col not in excluded]
 
# Define Feature Columns
features = cat_df + num_df
 
train_df = (train_df.fillna(0, subset=num_df)
...                     .fillna("null", subset=cat_df))
 
# Create columns aliases
catindex = add_suffix(cat_df, "_index")
 
# Fit StringIndexer
indexers = StringIndexer(inputCols=cat_df, outputCols= catindex)
indexers_model = indexers.fit(train_df)
 
# Transform to data
train_df_indexed = indexers_model.transform(train_df)
 
# Create columns aliases
catVector = add_suffix(cat_df, "_ohe")
 
# Fit OneHotEncoder
ohe = OneHotEncoder(inputCols=cat_index_cols, outputCols=cat_ohe_cols)
ohe_model = ohe.fit(train_df_indexed)
 
# Transform to data
train_df_ohed = ohe_model.transform(train_df_indexed)
 
 
 
 
# Functions for each type
# each tuple consist of: (function, column's suffix)
>>> num_funcs = [
          (F.mean, "_mean"),
          (F.stddev, "_std"),
          (F.min, "_min"),
          (F.max, "_max"),
]
 
>>> cat_funcs = [
          (F.count, "_count"),
          (F.last, "_last"),
          (F.countDistinct, "_nunique"),
]
 
# Arguments for .agg method
# each arg consist of: func(colname).alias(colname + suffix)
agg_num_args = [
...     func(col).alias(col + suffix) 
...     for col, (func, suffix) in itertools.product(num_cols, num_funcs)]
agg_cols_args = [
...     func(col).alias(col + suffix) 
...     for col, (func, suffix) in itertools.product(cat_ohe_cols, cat_funcs)]
 
# Combine numeric and categoric agg arguments
agg_args = agg_num_args + agg_cols_args
agg_args[0]
 
unused_cols = cat_cols + num_cols + cat_index_cols + cat_ohe_cols
 
# Apply the agg while also dropping unused columns
train_df_grouped = (train_df_ohed.groupBy("customer_ID")
...                                  .agg(*agg_cols_args)
...                                  .drop(*unused_cols))
 
train_df = train_df.join(F.broadcast(label_df), on="customer_ID")
 
va = VectorAssembler(
...     inputCols=train_joined_df.drop("customer_ID", "target").columns,
...     outputCol="features",
...     handleInvalid="error",
... )
 
train_ready_df = (va.transform(train_joined_df)
...                     .select(["customer_ID", "features", "target"])
...                     .persist(StorageLevel.DISK_ONLY))

# Create a label. =1 if default, =0 otherwise
train_ready_df= train_ready_df.withColumn("label", when(train_ready_df.target == 1, 1).otherwise(0))
 
# Split the data into training and test sets 
trainingData, testData = train_ready_df.randomSplit([0.7, 0.3])

# Create a LogisticRegression Estimator
lr = LogisticRegression() 

# Create the pipeline logistic regression (lr) is stage 0
 pipeline = Pipeline(stages=[lr])
 
# Create a grid to hold hyperparameters
 grid = ParamGridBuilder()
 grid = grid.addGrid(lr.regParam, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])
 grid = grid.addGrid(lr.elasticNetParam, [0, 0.5, 1])
 
# Build the parameter grid
 grid = grid.build()
 
# How many models to be tested
 print('Number of models to be tested: ', len(grid))
 Number of models to be tested:  18
 
# Create a BinaryClassificationEvaluator to evaluate how well the model works
 evaluator = BinaryClassificationEvaluator(metricName="areaUnderROC")
 
# Create the CrossValidator using the hyperparameter grid
 cv = CrossValidator(estimator=pipeline, 
 ... estimatorParamMaps=grid, 
 ... evaluator=evaluator, 
 ... numFolds=3)
 
# Train the models
 cv = cv.fit(trainingData)
 
# Test the predictions                                                      
 predictions = cv.transform(testData)
 
# Calculate AUC
 auc = evaluator.evaluate(predictions)
 print('AUC:', auc)                                                          
 
# Look at the parameters for the best model that was evaluated from the grid
 parammap = cv.bestModel.stages[0].extractParamMap()
 for p, v in parammap.items():
 ...     print(p, v) 

# Look at the parameters for the best model that was evaluated from the grid
  parammap = cv.bestModel.stages[0].extractParamMap()
  for p, v in parammap.items():
  ...     print(p, v)
  
# Look at the parameters for the best model that was evaluated from the grid
  parammap = cv.bestModel.stages[0].extractParamMap()
  for p, v in parammap.items():
  ...     print(p, v)
  cm = predictions.groupby('label').pivot('prediction').count().fillna(0).collect()
  def calculate_recall_precision(cm):                                         
  ...     tn = cm[0][1] # True Negative
  ...     fp = cm[0][2] # False Positive
  ...     fn = cm[1][1] # False Negative
  ...     tp = cm[1][2] # True Positive
  ...     precision = tp / ( tp + fp )
  ...     recall = tp / ( tp + fn )
  ...     accuracy = ( tp + tn ) / ( tp + tn + fp + fn )
  ...     f1_score = 2 * ( ( precision * recall ) / ( precision + recall ) )
  ...     return accuracy, precision, recall, f1_score

  print( calculate_recall_precision(cm) )
